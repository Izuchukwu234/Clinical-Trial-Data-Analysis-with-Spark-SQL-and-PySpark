{
  "nbformat": 4,
  "nbformat_minor": 2,
  "metadata": {
    "kernelspec": {
      "name": "synapse_pyspark",
      "display_name": "Synapse PySpark"
    },
    "language_info": {
      "name": "python"
    },
    "save_output": true,
    "synapse_widget": {
      "version": "0.1",
      "state": {}
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "#### Importing the neccessary libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "pip install folium keras-tuner"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "from pyspark.sql.functions import col, array\n",
        "import folium\n",
        "from folium.map import Marker\n",
        "from folium.plugins import HeatMap\n",
        "from functools import reduce\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "from pyspark.sql.functions import udf, mean, lit\n",
        "from pyspark.sql.types import DoubleType\n",
        "from geopy.distance import geodesic\n",
        "from pyspark.sql import Window\n",
        "import math\n",
        "from pyspark.sql import functions as F\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "from pyspark.ml.regression import RandomForestRegressor, DecisionTreeRegressor, DecisionTreeRegressionModel\n",
        "from pyspark.ml.evaluation import RegressionEvaluator\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "from pyspark.ml.feature import StandardScaler\n",
        "from pyspark.ml.tuning import ParamGridBuilder, TrainValidationSplit\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import tensorflow as tf\n",
        "import keras_tuner as kt\n",
        "from pyspark.sql.functions import to_timestamp, concat_ws, date_trunc, count\n",
        "from statsmodels.tsa.seasonal import seasonal_decompose\n",
        "from statsmodels.tsa.api import ExponentialSmoothing, SimpleExpSmoothing, Holt\n",
        "from statsmodels.tsa.holtwinters import HoltWintersResults\n",
        "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
        "from pyspark.sql import SparkSession"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "spark = SparkSession.builder \\\n",
        "    .appName(\"Optimized Spark Session\") \\\n",
        "    .config(\"spark.executor.memory\", \"4g\") \\\n",
        "    .config(\"spark.driver.memory\", \"4g\") \\\n",
        "    .config(\"spark.executor.cores\", \"4\") \\\n",
        "    .config(\"spark.driver.cores\", \"4\") \\\n",
        "    .config(\"spark.dynamicAllocation.enabled\", \"true\") \\\n",
        "    .config(\"spark.dynamicAllocation.minExecutors\", \"1\") \\\n",
        "    .config(\"spark.dynamicAllocation.maxExecutors\", \"10\") \\\n",
        "    .config(\"spark.sql.shuffle.partitions\", \"100\") \\\n",
        "    .getOrCreate()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "outputs": [],
      "metadata": {},
      "source": [
        "# Loading crime data\n",
        "df = (spark.read\n",
        "      .option(\"multiline\", \"true\")\n",
        "      .option(\"quote\", '\"')\n",
        "      .option(\"header\", \"true\")\n",
        "      .option(\"escape\", \"\\\\\")\n",
        "      .option(\"escape\", '\"')\n",
        "      .csv('abfss://raw@crimetsoppersnurture.dfs.core.windows.net/mercury-reports.csv'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Load infrastructure data\n",
        "infra_path = \"abfss://raw@crimetsoppersnurture.dfs.core.windows.net/National Infrastructure.csv\"\n",
        "infrastructure_df = spark.read.csv(infra_path, header = True, inferSchema = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Load CS_report data for time trends\n",
        "cs_report = \"abfss://raw@crimetsoppersnurture.dfs.core.windows.net/cs-reports-20170627-20240627.csv\"\n",
        "cs_report = spark.read.csv(cs_report, header = True, inferSchema = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Sample 20,000 random records from the file\n",
        "cs_report = cs_report.sample(withReplacement=False, fraction=20000/cs_report.count(), seed=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": false
      },
      "source": [
        "# Loading the CS_report dataset\n",
        "cs_report.show(truncate=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": false
      },
      "source": [
        "display(cs_report)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": false
      },
      "source": [
        "# Loading the crime report dataset\n",
        "df.show(truncate=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": false
      },
      "source": [
        "display(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": false
      },
      "source": [
        "# Loading the infrastructure dataset\n",
        "infrastructure_df.show(truncate=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": false
      },
      "source": [
        "display(infrastructure_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "df.count()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "infrastructure_df.count()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "cs_report.count()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": false
      },
      "source": [
        "summary_df = df.describe()\n",
        "display(summary_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "summary_infrastructure_df = infrastructure_df.describe()\n",
        "summary_infrastructure_df.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": false
      },
      "source": [
        "summary_cs_report = cs_report.describe()\n",
        "display(summary_cs_report)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "cs_report.printSchema()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "df.printSchema()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "infrastructure_df.printSchema()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### Data Cleaning and Preparation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "#### Checking for missing values in the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Listing columns\n",
        "col_ = df.columns\n",
        "\n",
        "# Creating a condition to check for null values\n",
        "null_cond = reduce(lambda acc, col_name: acc | col(col_name).isNull(), col_, col(col_[0]).isNull())\n",
        "\n",
        "# Filtering the df to get rows with any null\n",
        "df_null = df.filter(null_cond)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": false
      },
      "source": [
        "print(df_null.count())\n",
        "df_null.show(truncate=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Removing rows with null values in location columns with null values\n",
        "df = df.dropna(subset=[\"Location (Latitude)\", \"Location (Longitude)\"])\n",
        "df.count()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "infrastructure_df = infrastructure_df.dropna()\n",
        "infrastructure_df.count()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "#### Converting latitude and longitude to float"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "df = df.withColumn(\"latitude\", col(\"Location (Latitude)\").cast(\"float\"))\n",
        "df = df.withColumn(\"longitude\", col(\"Location (Longitude)\").cast(\"float\"))\n",
        "\n",
        "df.select(\"latitude\", \"longitude\").printSchema()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "infrastructure_df = infrastructure_df.withColumn(\"latitude_x\", col(\"Latitude\").cast(\"float\"))\n",
        "infrastructure_df = infrastructure_df.withColumn(\"longitude_x\", col(\"Longitude\").cast(\"float\"))\n",
        "\n",
        "infrastructure_df.select(\"latitude_x\", \"longitude_x\").printSchema()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "cs_report = cs_report.withColumn(\"latitude\", col(\"Location (Latitude)\").cast(\"float\"))\n",
        "cs_report = cs_report.withColumn(\"longitude\", col(\"Location (Longitude)\").cast(\"float\"))\n",
        "\n",
        "cs_report.select(\"latitude\", \"longitude\").printSchema()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "#### Crime Time Trends Near Infrastructure"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Define the distance calculation function\n",
        "def cal_distance(lat1, lon1, lat2, lon2):\n",
        "    return geodesic((lat1, lon1), (lat2, lon2)).kilometers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Register the distance calculation function as a UDF\n",
        "cal_distance_udf = udf(cal_distance, DoubleType())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Perform cross join to calculate distances between offenses and infrastructures\n",
        "joined_df = cs_report.crossJoin(infrastructure_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Calculate distances using the UDF\n",
        "distance_df = joined_df.withColumn('distance_km', cal_distance_udf(cs_report['latitude'], cs_report['longitude'],\n",
        "                                                                  infrastructure_df['latitude_x'], infrastructure_df['longitude_x']))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Filter offenses based on proximity to any infrastructure\n",
        "filtered_df = distance_df.filter(distance_df['distance_km'] <= 1.0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "cs_report_ = filtered_df.select(cs_report['Disseminated date'], cs_report['Disseminated time'],\n",
        "                                       cs_report['latitude'], cs_report['longitude'], cs_report['Offences'],\n",
        "                                       infrastructure_df['OrganisationName'], infrastructure_df['latitude_x'], infrastructure_df['longitude_x'],\n",
        "                                       distance_df['distance_km'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Combine the date and time columns into a single timestamp column\n",
        "cs_report_ = cs_report_.withColumn(\n",
        "    \"DateTimeString\", concat_ws(\" \", cs_report_[\"Disseminated date\"], cs_report_[\"Disseminated time\"])\n",
        ")\n",
        "\n",
        "# Convert the concatenated string to a timestamp\n",
        "cs_report_ = cs_report_.withColumn(\"DateTime\", to_timestamp(cs_report_[\"DateTimeString\"], \"dd/MM/yyyy HH:mm\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Drop the intermediate string column\n",
        "cs_report_ = cs_report_.drop(\"DateTimeString\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Aggregate by Month\n",
        "monthly_offences = cs_report_.groupBy(date_trunc(\"month\", \"DateTime\").alias(\"Month\")).agg(count(\"*\").alias(\"TotalOffences\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Convert to Pandas DataFrame for time series analysis\n",
        "monthly_offences_pd = monthly_offences.toPandas()\n",
        "monthly_offences_pd.set_index(\"Month\", inplace=True)\n",
        "monthly_offences_pd.sort_index(inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Plot the time series\n",
        "monthly_offences_pd.plot(title='Monthly Offences')\n",
        "plt.ylabel('Number of Offences')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Simple Moving Average\n",
        "monthly_offences_pd['SMA_3'] = monthly_offences_pd['TotalOffences'].rolling(window=3).mean()\n",
        "monthly_offences_pd['SMA_3'].plot()\n",
        "plt.title('Simple Moving Average (3)')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "monthly_offences_pd.dropna(inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "monthly_offences_pd = monthly_offences_pd[pd.notnull(monthly_offences_pd.index)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "monthly_offences_pd['TotalOffences']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Decompose the time series to identify trends and seasonality\n",
        "decomposition = seasonal_decompose(monthly_offences_pd['TotalOffences'], model='additive', period=12)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Plot the decomposition components separately\n",
        "plt.figure(figsize=(12, 8))\n",
        "\n",
        "# Plot observed data\n",
        "plt.subplot(411)\n",
        "plt.plot(monthly_offences_pd.index, monthly_offences_pd['TotalOffences'], label='Observed')\n",
        "plt.legend()\n",
        "\n",
        "# Plot trend component\n",
        "plt.subplot(412)\n",
        "plt.plot(monthly_offences_pd.index, decomposition.trend, label='Trend')\n",
        "plt.legend()\n",
        "\n",
        "# Plot seasonal component\n",
        "plt.subplot(413)\n",
        "plt.plot(monthly_offences_pd.index, decomposition.seasonal, label='Seasonal')\n",
        "plt.legend()\n",
        "\n",
        "# Plot residual component\n",
        "plt.subplot(414)\n",
        "plt.plot(monthly_offences_pd.index, decomposition.resid, label='Residual')\n",
        "plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "##### Holt-Winters Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Fit Holt-Winters model\n",
        "hw_model = ExponentialSmoothing(monthly_offences_pd['TotalOffences'], trend='add', seasonal='add', seasonal_periods=12)\n",
        "hw_model_fit = hw_model.fit()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Forecast next 72 months\n",
        "forecast = hw_model_fit.forecast(steps=72)\n",
        "forecast_index = pd.date_range(start=monthly_offences_pd.index[-1], periods=72, freq='MS')\n",
        "forecast_series = pd.Series(forecast, index=forecast_index)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Plot the forecast\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(monthly_offences_pd['TotalOffences'], label='Historical')\n",
        "plt.plot(forecast_series, label='Forecast', color='red')\n",
        "plt.title('Holt-Winters Forecast for Next 6 years')\n",
        "plt.xlabel('Date')\n",
        "plt.ylabel('Total Offences')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "##### ACF and PACF Plot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "plot_acf(monthly_offences_pd['TotalOffences'].dropna(), lags=20)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "plot_pacf(monthly_offences_pd['TotalOffences'].dropna(), lags=20)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "##### Visualization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Converting df to pandas\n",
        "df = df.toPandas()\n",
        "infrastructure_df = infrastructure_df.toPandas()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Scatter plot of the crime distribution\n",
        "plt.figure(figsize=(10, 8))\n",
        "plt.scatter(df['longitude'], df['latitude'], alpha=0.5, s=10, c='red')\n",
        "plt.title(\"Geographical Distribution of Crimes\")\n",
        "plt.xlabel(\"Longitude\")\n",
        "plt.ylabel(\"Latitude\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Scatter plot of the key infrastructure distribution\n",
        "plt.figure(figsize=(10, 8))\n",
        "plt.scatter(infrastructure_df['longitude_x'], infrastructure_df['latitude_x'], alpha=0.5, s=10, c='blue')\n",
        "plt.title(\"Geographical Distribution of the Key Infrastructure\")\n",
        "plt.xlabel(\"Longitude\")\n",
        "plt.ylabel(\"Latitude\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Creating a map centered around the mean latitude and longitude of the crime\n",
        "map_center = [df['latitude'].mean(), df['longitude'].mean()]\n",
        "crime_map = folium.Map(location = map_center, zoom_start = 10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Adding heatmap to the map\n",
        "heat_data = [[row['latitude'], row['longitude']] for index, row in df.iterrows()]\n",
        "HeatMap(heat_data).add_to(crime_map)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "crime_map"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Creating a map centered around the mean latitude and longitude of infrastructure\n",
        "map_center = [infrastructure_df['latitude_x'].mean(), infrastructure_df['longitude_x'].mean()]\n",
        "infrastructure_map = folium.Map(location = map_center, zoom_start = 10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Adding heatmap to the map\n",
        "heat_data = [[row['latitude_x'], row['longitude_x']] for index, row in infrastructure_df.iterrows()]\n",
        "HeatMap(heat_data).add_to(infrastructure_map)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "infrastructure_map"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Adding points to the map with cross marker\n",
        "for idx, row in df.iterrows():\n",
        "    icon = folium.DivIcon(\n",
        "        html='<div style=\"font-size: 18px; color: blue;\">o</div>',\n",
        "        icon_size=(18, 18),\n",
        "        icon_anchor=(6, 6)\n",
        "    )\n",
        "    tooltip_info = row['Offences']\n",
        "    Marker([row['latitude'], row['longitude']],\n",
        "           icon=icon,\n",
        "           tooltip=tooltip_info\n",
        "          ).add_to(crime_map)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "crime_map"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Adding points to the map with cross marker\n",
        "for idx, row in infrastructure_df.iterrows():\n",
        "    icon = folium.DivIcon(\n",
        "        html='<div style=\"font-size: 18px; color: red;\">+</div>',\n",
        "        icon_size=(18, 18),\n",
        "        icon_anchor=(6, 6)\n",
        "    )\n",
        "    tooltip_info = row['OrganisationName']\n",
        "    Marker([row['latitude_x'], row['longitude_x']],\n",
        "           icon=icon,\n",
        "           tooltip=tooltip_info\n",
        "          ).add_to(infrastructure_map)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "infrastructure_map"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Creating a map centered around the mean latitude and longitude of all data points\n",
        "map_center = [\n",
        "    (df['latitude'].mean() + infrastructure_df['latitude_x'].mean()) / 2,\n",
        "    (df['longitude'].mean() + infrastructure_df['longitude_x'].mean()) / 2\n",
        "]\n",
        "combined_map = folium.Map(location=map_center, zoom_start=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Adding crime data with \"o\" markers\n",
        "for idx, row in df.iterrows():\n",
        "    icon = folium.DivIcon(\n",
        "        html='<div style=\"font-size: 18px; color: blue;\">o</div>',\n",
        "        icon_size=(18, 18),\n",
        "        icon_anchor=(6, 6)\n",
        "    )\n",
        "    folium.Marker(\n",
        "        location=[row['latitude'], row['longitude']],\n",
        "        icon=icon,\n",
        "        tooltip=f\"Crime Location: {row['Offences']} ({row['latitude']}, {row['longitude']})\"\n",
        "    ).add_to(combined_map)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Adding infrastructure data with \"+\" markers\n",
        "for idx, row in infrastructure_df.iterrows():\n",
        "    icon = folium.DivIcon(\n",
        "        html='<div style=\"font-size: 18px; color: red;\">+</div>',\n",
        "        icon_size=(18, 18),\n",
        "        icon_anchor=(6, 6)\n",
        "    )\n",
        "    folium.Marker(\n",
        "        location=[row['latitude_x'], row['longitude_x']],\n",
        "        icon=icon,\n",
        "        tooltip=f\"Infrastructure: {row['OrganisationName']} ({row['latitude_x']}, {row['longitude_x']})\"\n",
        "    ).add_to(combined_map)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "combined_map"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Creating a pivot table to count offenses by Contact Type and Method\n",
        "contact_counts = df.pivot_table(index='Contact Type', columns='Contact Method', values='Offences', aggfunc='count', fill_value=0)\n",
        "\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.heatmap(contact_counts, cmap='viridis', annot=True, fmt='g')\n",
        "plt.title(\"Analysis of Crime Reports by Contact Type and Method\")\n",
        "plt.xlabel(\"Contact Method\")\n",
        "plt.ylabel(\"Contact Type\")\n",
        "plt.xticks(rotation=45)\n",
        "plt.yticks(rotation=0)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### Spatial Analysis and Feature Engineering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Converting Pandas DataFrame back to PySpark DataFrame\n",
        "df = spark.createDataFrame(df)\n",
        "infrastructure_df = spark.createDataFrame(infrastructure_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Cross joining crime data with infrastructure data\n",
        "df = df.crossJoin(infrastructure_df.select(\"latitude_x\", \"longitude_x\", \"OrganisationName\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": false
      },
      "source": [
        "# Calculate distance to each infrastructure point\n",
        "df = df.withColumn(\"distance\", cal_distance_udf(df[\"latitude\"], df[\"longitude\"], \n",
        "                                                        infrastructure_df[\"latitude_x\"], infrastructure_df[\"longitude_x\"]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": false
      },
      "source": [
        "df.show(truncate = False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Filter to find the nearest infrastructure\n",
        "windowSpec = Window.partitionBy(\"ISR\").orderBy(\"distance\")\n",
        "df_near = df.withColumn(\"row_number\", F.row_number().over(windowSpec)).filter(F.col(\"row_number\") == 1).drop(\"row_number\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": false
      },
      "source": [
        "df.show(truncate = False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Collect data for plotting\n",
        "crime_points = df_near.select(\"latitude\", \"longitude\", \"latitude_x\", \"longitude_x\", \"distance\", \"OrganisationName\", \"Offences\").collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Creating a map centered around the mean latitude and longitude of all data points\n",
        "map_center_1 = [\n",
        "    (df_near.agg(mean(\"latitude\")).collect()[0][0] + df_near.agg(mean(\"latitude_x\")).collect()[0][0]) / 2,\n",
        "    (df_near.agg(mean(\"longitude\")).collect()[0][0] + df_near.agg(mean(\"longitude_x\")).collect()[0][0]) / 2\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "combined_map_1 = folium.Map(location=map_center_1, zoom_start=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Adding crime data with \"o\" markers\n",
        "for row in crime_points:\n",
        "    icon = folium.DivIcon(\n",
        "        html='<div style=\"font-size: 14px; color: blue;\">o</div>',\n",
        "        icon_size=(14, 14),\n",
        "        icon_anchor=(6, 6)\n",
        "    )\n",
        "    folium.Marker(\n",
        "        location=[row['latitude'], row['longitude']],\n",
        "        icon=icon,\n",
        "        tooltip=f\"Crime Location: {row['Offences']} ({row['latitude']}, {row['longitude']})\"\n",
        "    ).add_to(combined_map_1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Adding infrastructure data with \"+\" markers\n",
        "for row in crime_points:\n",
        "    icon = folium.DivIcon(\n",
        "        html='<div style=\"font-size: 14px; color: red;\">+</div>',\n",
        "        icon_size=(14, 14),\n",
        "        icon_anchor=(6, 6)\n",
        "    )\n",
        "    folium.Marker(\n",
        "        location=[row['latitude_x'], row['longitude_x']],\n",
        "        icon=icon,\n",
        "        tooltip=f\"Infrastructure: {row['OrganisationName']} ({row['latitude_x']}, {row['longitude_x']})\"\n",
        "    ).add_to(combined_map_1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Drawing lines and adding distance labels\n",
        "for row in crime_points:\n",
        "    folium.PolyLine(\n",
        "        locations=[[row['latitude'], row['longitude']], [row['latitude_x'], row['longitude_x']]],\n",
        "        color='black'\n",
        "    ).add_to(combined_map_1)\n",
        "    mid_lat = (row['latitude'] + row['latitude_x']) / 2\n",
        "    mid_lon = (row['longitude'] + row['longitude_x']) / 2\n",
        "    folium.Marker(\n",
        "        location=[mid_lat, mid_lon],\n",
        "        icon=folium.DivIcon(html=f'<div style=\"font-size: 12px; color: black;\">{row[\"distance\"]:.2f} km</div>'),\n",
        "    ).add_to(combined_map_1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "combined_map_1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "##### Correlation Matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Convert df_near to Pandas DataFrame for correlation analysis\n",
        "df_near_pd = df_near.select(\"latitude\", \"longitude\", \"distance\").toPandas()\n",
        "\n",
        "# Compute correlation matrix\n",
        "corr_matrix = df_near_pd.corr()\n",
        "\n",
        "# Plot the correlation matrix\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt='.2f')\n",
        "plt.title('Correlation Matrix')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Print the correlation matrix\n",
        "print(corr_matrix)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Count the number of crimes within each distance for each infrastructure\n",
        "agg_df = df_near.groupBy(\"OrganisationName\", \"distance\").agg(F.count(\"*\").alias(\"crime_count\"))\n",
        "\n",
        "# Sum the crime_count for each infrastructure\n",
        "final_agg_df = agg_df.groupBy(\"OrganisationName\").agg(F.sum(\"crime_count\").alias(\"total_crime_count\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "final_agg_df.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "final_df = df.join(final_agg_df, on=\"OrganisationName\", how=\"left\").dropna()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Define the feature columns\n",
        "feature_columns = [\"latitude\", \"longitude\", \"distance\"]\n",
        "\n",
        "# Assemble the features\n",
        "assembler = VectorAssembler(inputCols=feature_columns, outputCol=\"features\")\n",
        "final_df = assembler.transform(final_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### Predictive Modelling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Split the data into training and testing sets\n",
        "train_df, test_df = final_df.randomSplit([0.8, 0.2], seed=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "##### Scaling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaled_features\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Fit and transform the training data\n",
        "scaler_model = scaler.fit(train_df)\n",
        "train_df = scaler_model.transform(train_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Transform the testing data\n",
        "test_df = scaler_model.transform(test_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "#### Random Forest"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Train Random Forest model\n",
        "rf = RandomForestRegressor(featuresCol=\"features\", labelCol=\"total_crime_count\")\n",
        "rf_model = rf.fit(train_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 94,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Make predictions\n",
        "predictions = rf_model.transform(test_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 95,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Evaluate model\n",
        "evaluator = RegressionEvaluator(labelCol=\"total_crime_count\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
        "rf_rmse = evaluator.evaluate(predictions)\n",
        "print(f\"Root Mean Squared Error (RMSE) on test data = {rf_rmse}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 96,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Convert the PySpark DataFrame to a Pandas DataFrame for visualization\n",
        "predictions_pd = predictions.select(\"total_crime_count\", \"prediction\").toPandas()\n",
        "\n",
        "# Compute residuals\n",
        "predictions_pd['residuals'] = predictions_pd['total_crime_count'] - predictions_pd['prediction']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 97,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Plot Residuals\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.scatterplot(x=predictions_pd['prediction'], y=predictions_pd['residuals'])\n",
        "plt.axhline(0, color='red', linestyle='--')\n",
        "plt.xlabel('Predicted Values')\n",
        "plt.ylabel('Residuals')\n",
        "plt.title('Residual Plot')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 98,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Predicted vs. Actual Plot\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.scatterplot(x=predictions_pd['total_crime_count'], y=predictions_pd['prediction'])\n",
        "plt.plot([predictions_pd['total_crime_count'].min(), predictions_pd['total_crime_count'].max()], \n",
        "         [predictions_pd['total_crime_count'].min(), predictions_pd['total_crime_count'].max()], \n",
        "         color='red', linestyle='--')\n",
        "plt.xlabel('Actual Values')\n",
        "plt.ylabel('Predicted Values')\n",
        "plt.title('Predicted vs. Actual')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 99,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Histogram of Residuals\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.histplot(predictions_pd['residuals'], kde=True)\n",
        "plt.xlabel('Residuals')\n",
        "plt.title('Distribution of Residuals')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "#### Decision Tree"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Train Decision Tree model\n",
        "dt = DecisionTreeRegressor(featuresCol=\"scaled_features\", labelCol=\"total_crime_count\")\n",
        "dt_model = dt.fit(train_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 101,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Make predictions\n",
        "predictions = dt_model.transform(test_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 102,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Evaluate model\n",
        "evaluator = RegressionEvaluator(labelCol=\"total_crime_count\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
        "dt_rmse = evaluator.evaluate(predictions)\n",
        "print(f\"Root Mean Squared Error (RMSE) on test data = {dt_rmse}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "#### TensorFlow Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 103,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Convert Spark DataFrame to Pandas DataFrame\n",
        "train_df_pd = train_df.select(\"features\", \"total_crime_count\").toPandas()\n",
        "test_df_pd = test_df.select(\"features\", \"total_crime_count\").toPandas()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 104,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Extract values from DenseVector and convert to NumPy arrays\n",
        "train_features = train_df_pd['features'].apply(lambda x: x.toArray()).values.tolist()\n",
        "train_labels = train_df_pd['total_crime_count'].values.tolist()\n",
        "test_features = test_df_pd['features'].apply(lambda x: x.toArray()).values.tolist()\n",
        "test_labels = test_df_pd['total_crime_count'].values.tolist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 105,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Convert Pandas DataFrame to TensorFlow Dataset\n",
        "train_ds = tf.data.Dataset.from_tensor_slices((train_features, train_labels)).cache()\n",
        "test_ds = tf.data.Dataset.from_tensor_slices((test_features, test_labels)).cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 106,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Define the TensorFlow model\n",
        "def create_model():\n",
        "    model = Sequential([\n",
        "        Dense(10, activation='relu', input_shape=(len(feature_columns),)),\n",
        "        Dense(5, activation='relu'),\n",
        "        Dense(1)\n",
        "    ])\n",
        "    model.compile(optimizer=Adam(), loss='mean_squared_error', metrics=[root_mean_squared_error])\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 107,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Define RMSE metric\n",
        "def root_mean_squared_error(y_true, y_pred):\n",
        "    return tf.sqrt(tf.reduce_mean(tf.square(y_pred - y_true)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 108,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Train the model using the TensorFlow Dataset\n",
        "model = create_model()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 109,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 110,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Train the model with checkpoints and early stopping\n",
        "history = model.fit(\n",
        "    train_ds.batch(80),\n",
        "    epochs=10,\n",
        "    validation_data=test_ds.batch(80)\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 111,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Plot training history\n",
        "def plot_history(history):\n",
        "    plt.figure(figsize=(12, 6))\n",
        "\n",
        "    # Plot loss\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(history.history['loss'], label='Training Loss')\n",
        "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.title('Training and Validation Loss')\n",
        "    plt.legend()\n",
        "\n",
        "    # Plot RMSE\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(history.history['root_mean_squared_error'], label='Training RMSE')\n",
        "    plt.plot(history.history['val_root_mean_squared_error'], label='Validation RMSE')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('RMSE')\n",
        "    plt.title('Training and Validation RMSE')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 112,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "plot_history(history)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 113,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Evaluate the model\n",
        "loss, rmse = model.evaluate(test_ds.batch(80))\n",
        "print(f\"Root Mean Square Error (RMSE) on test data: {rmse}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "##### Hyperparameter tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Define the model-building function\n",
        "def build_model(hp):\n",
        "    model = Sequential()\n",
        "    model.add(Dense(units=hp.Int('units_layer1', min_value=8, max_value=64, step=8),\n",
        "                    activation='relu', input_shape=(len(feature_columns),)))\n",
        "    model.add(Dense(units=hp.Int('units_layer2', min_value=4, max_value=32, step=4),\n",
        "                    activation='relu'))\n",
        "    model.add(Dense(1))\n",
        "    model.compile(optimizer=Adam(hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4])),\n",
        "                  loss='mean_squared_error',\n",
        "                  metrics=[root_mean_squared_error])\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Initialize the tuner with explicit Objective definition\n",
        "tuner = kt.RandomSearch(\n",
        "    build_model,\n",
        "    objective=kt.Objective('val_root_mean_squared_error', direction='min'),\n",
        "    max_trials=1,\n",
        "    executions_per_trial=1,\n",
        "    directory='my_dir',\n",
        "    project_name='intro_to_kt'\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 116,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Perform the search\n",
        "tuner.search(train_ds.batch(80),\n",
        "             epochs=10,\n",
        "             validation_data=test_ds.batch(80))\n",
        "\n",
        "# Get the optimal hyperparameters\n",
        "best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
        "\n",
        "print(f\"\"\"\n",
        "The optimal number of units in the first dense layer is {best_hps.get('units_layer1')} and the second dense layer is {best_hps.get('units_layer2')}.\n",
        "The optimal learning rate for the optimizer is {best_hps.get('learning_rate')}.\n",
        "\"\"\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 117,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Build the model with the optimal hyperparameters and train it\n",
        "model = tuner.hypermodel.build(best_hps)\n",
        "history = model.fit(train_ds.batch(80),\n",
        "                    epochs=10,\n",
        "                    validation_data=test_ds.batch(80))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 119,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Plot training history\n",
        "def plot_history(history):\n",
        "    plt.figure(figsize=(12, 6))\n",
        "\n",
        "    # Plot loss\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(history.history['loss'], label='Training Loss')\n",
        "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.title('Training and Validation Loss')\n",
        "    plt.legend()\n",
        "\n",
        "    # Plot RMSE\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(history.history['root_mean_squared_error'], label='Training RMSE')\n",
        "    plt.plot(history.history['val_root_mean_squared_error'], label='Validation RMSE')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('RMSE')\n",
        "    plt.title('Training and Validation RMSE')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "plot_history(history)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 120,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Evaluate the loaded model\n",
        "loss, rmse = model.evaluate(test_ds.batch(80))\n",
        "print(f\"Root Mean Square Error (RMSE) on test data: {rmse}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 145,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Evaluate the loaded model and make predictions\n",
        "predictions = model.predict(test_ds.batch(80))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 149,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Extract true values from the dataset\n",
        "true_values = []\n",
        "\n",
        "# Iterate through the dataset and collect true values\n",
        "for x, y in test_ds:\n",
        "    # Convert scalar tensors to arrays\n",
        "    true_values.append(np.array(y))\n",
        "\n",
        "# Convert the list to a NumPy array\n",
        "true_values = np.array(true_values)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 150,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Convert predictions and true values to Pandas DataFrame for visualization\n",
        "predictions_df = pd.DataFrame({\n",
        "    'True Values': true_values.flatten(),\n",
        "    'Predicted Values': predictions.flatten()\n",
        "})\n",
        "predictions_df['Residuals'] = predictions_df['True Values'] - predictions_df['Predicted Values']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 151,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Plot: Predicted vs. Actual Values\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.scatterplot(x=predictions_df['True Values'], y=predictions_df['Predicted Values'])\n",
        "plt.plot([predictions_df['True Values'].min(), predictions_df['True Values'].max()], \n",
        "         [predictions_df['True Values'].min(), predictions_df['True Values'].max()], \n",
        "         color='red', linestyle='--')\n",
        "plt.xlabel('True Values')\n",
        "plt.ylabel('Predicted Values')\n",
        "plt.title('Predicted vs. Actual Values')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 152,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Plot: Residuals\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.scatterplot(x=predictions_df['Predicted Values'], y=predictions_df['Residuals'])\n",
        "plt.axhline(0, color='red', linestyle='--')\n",
        "plt.xlabel('Predicted Values')\n",
        "plt.ylabel('Residuals')\n",
        "plt.title('Residual Plot')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 153,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Plot: Histogram of Residuals\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.histplot(predictions_df['Residuals'], bins=30, kde=True)\n",
        "plt.xlabel('Residuals')\n",
        "plt.ylabel('Frequency')\n",
        "plt.title('Histogram of Residuals')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 154,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Print the RMSE\n",
        "print(f\"Root Mean Square Error (RMSE) on test data: {rmse}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 133,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Save the model with overwrite option\n",
        "model.save('my_model.h5', overwrite=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "##### Predicting with the model for DT and RF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 134,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Saving the model using Spark's save method\n",
        "model_path = \"dt_model_spark\"\n",
        "dt_model.write().overwrite().save(model_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 135,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Loading the model using Spark's load method\n",
        "dt_model_loaded = DecisionTreeRegressionModel.load(model_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 136,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Define a function to prepare sample data\n",
        "def prepare_sample_data():\n",
        "    samp_data = {\n",
        "        'latitude': [50.540740002664144],\n",
        "        'longitude': [-2.04143996166132524],\n",
        "        'distance': [5.46881194025045]\n",
        "    }\n",
        "    sample_ = pd.DataFrame(samp_data)\n",
        "    return sample_\n",
        "\n",
        "# Prepare the sample data\n",
        "sample_ = prepare_sample_data()\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 137,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Convert the sample data to the format expected by the model\n",
        "sample_spark_df = spark.createDataFrame(sample_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 138,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Assemble the features into a single vector\n",
        "assembler = VectorAssembler(inputCols=sample_spark_df.columns, outputCol=\"features\")\n",
        "sample_spark_df = assembler.transform(sample_spark_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 139,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Scale the features using the previously fitted scaler model\n",
        "sample_spark_df = scaler_model.transform(sample_spark_df)\n",
        "\n",
        "# Select only the scaled_features column\n",
        "sample_features = sample_spark_df.select(\"scaled_features\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 140,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Predict function\n",
        "def predict_crime(model, input_data):\n",
        "    # Model expects a DataFrame\n",
        "    prediction = model.transform(input_data).collect()[0]['prediction']\n",
        "    return prediction\n",
        "\n",
        "prediction = predict_crime(dt_model_loaded, sample_features)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 141,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Print the prediction\n",
        "print(f'Predicted Crime Count Near Infrastructure: {prediction:.0f}')\n",
        "\n",
        "if prediction >= 10:\n",
        "    print(\"**Warning: High potential threat to infrastructure!**\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "##### Predicting with the model for TensorFlow Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 142,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Define a function to prepare sample data\n",
        "def prepare_sample_data():\n",
        "    samp_data = {\n",
        "        'latitude': [51.37999725341797],\n",
        "        'longitude': [-0.406042069196701],\n",
        "        'distance': [20.46881194025045]\n",
        "    }\n",
        "    sample_ = pd.DataFrame(samp_data)\n",
        "    return sample_\n",
        "\n",
        "sample_ = prepare_sample_data()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 143,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Convert the sample data to the format expected by the model\n",
        "sample_spark_df = spark.createDataFrame(sample_)\n",
        "\n",
        "# Assemble the features into a single vector\n",
        "assembler = VectorAssembler(inputCols=sample_spark_df.columns, outputCol=\"features\")\n",
        "sample_spark_df = assembler.transform(sample_spark_df)\n",
        "\n",
        "# Scale the features using the previously fitted scaler model\n",
        "sample_spark_df = scaler_model.transform(sample_spark_df)\n",
        "\n",
        "# Select only the scaled_features column\n",
        "sample_features = sample_spark_df.select(\"scaled_features\").collect()\n",
        "\n",
        "# Convert to a NumPy array\n",
        "sample_features_np = np.array([row['scaled_features'] for row in sample_features])\n",
        "\n",
        "# Load the TensorFlow model\n",
        "loaded_model = tf.keras.models.load_model('my_model.h5', custom_objects={'root_mean_squared_error': root_mean_squared_error})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 144,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Predict function for TensorFlow model\n",
        "def predict_crime_tensorflow(model, input_data):\n",
        "    prediction = model.predict(input_data)\n",
        "    return prediction[0][0]\n",
        "\n",
        "# Make predictions with the TensorFlow model\n",
        "prediction = predict_crime_tensorflow(loaded_model, sample_features_np)\n",
        "\n",
        "# Print the prediction\n",
        "print(f'Predicted Crime Count Near Infrastructure: {prediction:.0f}')\n",
        "\n",
        "if prediction >= 10:\n",
        "    print(\"**Warning: High potential threat to infrastructure!**\")"
      ]
    }
  ]
}